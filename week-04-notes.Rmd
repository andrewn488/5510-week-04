---
title: "Week-04-Notes"
author: "Andrew Nalundasan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview Video



# Data Collection

+ Data is our essential tool as business analysts
+ Data COllection

    + Non-technical collection
        + Survey / Interviews
    + Technical
        + Data from software
        + Sensor logs
        + Automated reports
        
+ Data files

    + Structured
        + CSV / Excel / Logs with key-value pairs
    + Semi-structured
        + XML
        + JSON
        + HTML
    + Unstructured
        + Logs
        + Text
        + Binary

+ Data Ingestion

    + Often done with programming scripts
    + Customized open-source software
    + Purchased private software
    + Push & Pull
    + Publish & Subscribe
    
+ ETL <- Extract / Transform / Load

    + Extract <- through APIs, scripts, software
    + Transform <- preprocess the data
    + Load <- in storage
    
+ Batch vs. real-time

    + batch <- collected within time increments
        + delay of receiving of data (not real-time)
    + real-time <- occurs by extracting data and displaying it in real time
        + expensive to run
        
+ 5 V's

    + Volume / Velocity / Variety / Veracity (meaningful and accurate, clean of noise) / Value
    
+ Data Bodies of Water

    + Data Puddle <- single-purpose data mart built for a single project or team
    + Data Pond <- collection of data puddles. Not efficient method of data storage since it requires high level of IT participation
    + Data Lake <- supports self-service (business users can find and user data sets they want without IT dept) and contains data even if not being currently used by a project
        + can be accessed from business users, IT, end users, et al. 
    + Data Ocean <- expands self-service model of data lake to all enterprise data
    
![](bodies_of_water.png)

+ Hadoop

    + Original solution to managing data
    + Apache Spark
    + Databricks
    + AWS
    + Google Cloud Platform (GCP), Google Big Query
    + MapR
    
+ why do we care?

    + often in smaller companies, it is up to the data analyst to act as a data engineer
    + most big companies have separate functions between data engineer vs. data analyst
    + important for the analyst to understand what they're handling

# Data Exploration 

+ Data Types

    + Structured <- labeled data, best for classification / supervised learning
        + Dimensionality <- too many columns / features?
        + Sparsity <- only presence counts (sparse / missing data) 
        + Resolution <- patterns depend on the scale
        + Distribution <- centrality and dispersion (understanding the data from a distributed perspective)
        + Attributes <- characteristics / features
            + Nominal <- categories, states
              + Binary <- nominal with 2 states
                + symmetric / asymmetric
            + Numeric <_ quantitative
              + Interval <- meaningful differences
              + Ratio <- same as interval, but also 0.0 means null()
            + Ordinal <- values have meaningful order
    + Unstructured  <- unlabeled data, best for clustering / unsupervised learning
    + Semi-structured data <- with labels but does not officially reside in tabular structure
    
+ Statistical Descriptions

    + Boxplot <- graphic display of 5-number summary
    + Histogram <- x-axis are values, y-axis represents frequencies
    + Scatter plot <- each pair of values is a pair of coords and plotted as points in the plane
    
+ Quantile-Quantile (Q-Q) Plot

    + Univariate distribution against the corresponding quantiles of another
    + Allows a view into whether there is a shift in going from one distribution to another
    + Similar to putting 2 histograms together to glean differences between the two
    
+ Similarity <- numerical measure of how alike data objects are, often between [0, 1]
+ Dissimilarity <- numerical measure of how different data objects are, lower when more objects alike

+ Distance measures

    + Manhattan <- number of bits that are different between two binary vectors
    + Euclidean <- point to point

# Data Preprocessing I

+ Measures of data quality: 

    + Must have all these characteristics to provide value: 
    + Accuracy
    + Completeness
    + Consistency
    +Timeliness
    + Believability
    + Interpretability

+ Data Cleaning <- fill in missing values, smooth noisy data, identify or remove outliers, and resolve inconsistencies

    + Data in the real world is dirty: lots of incorrect data, faulty instruments, human error, etc.
    + Incomplete <- lacking attribute values, lacking certain attributes of interest, or containing only aggregate data
        + Occupation="" (missing data)
    + Noisy <- containing noise, errors, or outliers
        + Salary = "-10" (an error)
    + Inconsistent <- containing discrepancies in codes or names
        + Age = "42", Birthday = "03/07/2010"
    + Intentional (disguised missing data)
        + Jan. 1 as everyone's birthday?
    + Handling missing data
        + Ignore
            + less work but could be invalid
        + Fill in missing values manually
            + gives control but time consuming
        + Fill in missing values automatically with
            + Global constant
            + Attribute mean
            + Attribute mean of all samples belonging to the same class
        + Binning
            + first sort data and partition into (equal-frequency) bins
            + then one can smooth by bin means, bin median, or bin boundries
        + Regression
            + smooth by fitting into a regression function
        + Clustering
            + Detect and remove outliers
        + Combined computer and human inspection
            + detect suspicious values and check by human

+ Data integration <- integration of multiple databases, data cubes, or files

    + Data Integration <- All about combining data from multiple sources into a coherent store
    + Schema integration <- integrate metadata from different sources
    + Entity identification problem
        + Identify real world entities from multiple data sources (Bill Clinton = William Clinton)
    + Detecting and resolving data value conflicts
        + For the same real-world entity, attribute values from different sources are different
        + Possible reasons: different representations, different scales (metric vs. imperial units)
        + Redundant attributes may be able to be detected by correlation analysis or covariance analysis
        + Correlation analysis: Chi-Square test - The larger the X**2 value, the more likely the variables are related
        + Correlation Coefficient (Pearson's Coefficient)
            + Higher <- stronger correlation
            + Negative <- weaker correlation
            
# Data Preprocessing II

+ Data Reduction <- dimensionality reduction, numeriosity reduction, data compression
    
    + To reduce computation time on very large datasets
    + Dimensionality reduction <- remove unimportant attributes
        + Principal Components Analysis (PCA)
            + Fine a projection that captures the largest amount of variation in data in "components"
            + Original data are projected onto a much smaller space by removing the components with the smallest variance, resulting in dimensionality reduction. We find the eigenvectors of the covariance matrix, and these eigenvectors define the new space
            + Find a projection that captures the largest amount of variation
        + Feature subset selection, feature creation
        + Curse of dimensionality
            + Sparse data causes density and sitance between points to be less meaningful
            + If we have more features than observations, we run the risk of massively overfitting our model
            + Too many dimensions causes every observation in your dataset to appear equidistant from the others. If distances are approximately equal, no meaningful clusters can be formed
    + Numerosity reduction aka "Data Reduction"
        + Regression and Log-Linear Models
        + Histograms, clustering, sampling
        + Numerosity Reduction <- data replaced by smaller representative models. Can be parametric or non-parametric
            + Parametric:
                + Linear regression <- data modeled to fit a straight line; often uses the least-square method to fit the line
                + Multiple regression <- allows a response variable Y to be modeled as a linear function of multidimensional feature vector
            + Non-parametric Methods
                + Histograms <- binning into different groups
                + Clustering
                + Sampling <- obtaining small sample to represent whole data set
                + Simple random sampling <- equal prbability of selecting any particular item
                + Stratified sampling <- partition the data set, and draw samples from each partition (proportionally - approximately the same percentage of the data), used in conjunction with skewed data
                + Sampling without replacement
                    + Once an object is selected, it is removed from the population. It cannot be sampled again
                + Sampling with replacement
                    + Selected object is not removed from the population, it may be sampled again, can get a lot of redundancy
                

+ Data transformation and data discretization <- normalization, concept hierarchy generation

    + Function that maps the entire set of values of a given attribute to a new set of replacement values that each old value can be identified with one of the new values
    + Methods
        + Smoothing <- removes noise from data
        + Attribute / Feature construction <- new attributes constructed from the given ones
        + Aggregation <- summarization, data cube construction
        + Normalization <- scaled to fall within a smaller, specified range
            + min-max normalization
            + z-score normalization
            + normalization by decimal scaling
        + Discretization <- concept hierarchy climbing





